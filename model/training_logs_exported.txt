(base) PS C:\Users\ML_Team\Documents\Projects\MelodySynthesis> python train.py
2025-03-19 19:43:57.544023: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-19 19:43:59.168475: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Train inputs shape: (997066, 64, 45), Targets shape: (997066,)
Val inputs shape: (75294, 64, 45), Targets shape: (75294,)
2025-03-19 19:44:13.379119: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                  ┃ Output Shape              ┃         Param # ┃ Connected to               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)      │ (None, 64, 45)            │               0 │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense (Dense)                 │ (None, 64, 256)           │          11,776 │ input_layer[0][0]          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization           │ (None, 64, 256)           │             512 │ dense[0][0]                │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ multi_head_attention          │ (None, 64, 256)           │       2,103,552 │ layer_normalization[0][0], │
│ (MultiHeadAttention)          │                           │                 │ layer_normalization[0][0]  │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_1 (Dropout)           │ (None, 64, 256)           │               0 │ multi_head_attention[0][0] │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add (Add)                     │ (None, 64, 256)           │               0 │ dropout_1[0][0],           │
│                               │                           │                 │ dense[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization_1         │ (None, 64, 256)           │             512 │ add[0][0]                  │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_1 (Dense)               │ (None, 64, 512)           │         131,584 │ layer_normalization_1[0][… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_2 (Dense)               │ (None, 64, 256)           │         131,328 │ dense_1[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_2 (Dropout)           │ (None, 64, 256)           │               0 │ dense_2[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add_1 (Add)                   │ (None, 64, 256)           │               0 │ dropout_2[0][0], add[0][0] │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization_2         │ (None, 64, 256)           │             512 │ add_1[0][0]                │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ multi_head_attention_1        │ (None, 64, 256)           │       2,103,552 │ layer_normalization_2[0][… │
│ (MultiHeadAttention)          │                           │                 │ layer_normalization_2[0][… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_4 (Dropout)           │ (None, 64, 256)           │               0 │ multi_head_attention_1[0]… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add_2 (Add)                   │ (None, 64, 256)           │               0 │ dropout_4[0][0],           │
│                               │                           │                 │ add_1[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization_3         │ (None, 64, 256)           │             512 │ add_2[0][0]                │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_3 (Dense)               │ (None, 64, 512)           │         131,584 │ layer_normalization_3[0][… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_4 (Dense)               │ (None, 64, 256)           │         131,328 │ dense_3[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_5 (Dropout)           │ (None, 64, 256)           │               0 │ dense_4[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add_3 (Add)                   │ (None, 64, 256)           │               0 │ dropout_5[0][0],           │
│                               │                           │                 │ add_2[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization_4         │ (None, 64, 256)           │             512 │ add_3[0][0]                │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ multi_head_attention_2        │ (None, 64, 256)           │       2,103,552 │ layer_normalization_4[0][… │
│ (MultiHeadAttention)          │                           │                 │ layer_normalization_4[0][… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_7 (Dropout)           │ (None, 64, 256)           │               0 │ multi_head_attention_2[0]… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add_4 (Add)                   │ (None, 64, 256)           │               0 │ dropout_7[0][0],           │
│                               │                           │                 │ add_3[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ layer_normalization_5         │ (None, 64, 256)           │             512 │ add_4[0][0]                │
│ (LayerNormalization)          │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_5 (Dense)               │ (None, 64, 512)           │         131,584 │ layer_normalization_5[0][… │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_6 (Dense)               │ (None, 64, 256)           │         131,328 │ dense_5[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout_8 (Dropout)           │ (None, 64, 256)           │               0 │ dense_6[0][0]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ add_5 (Add)                   │ (None, 64, 256)           │               0 │ dropout_8[0][0],           │
│                               │                           │                 │ add_4[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ get_item (GetItem)            │ (None, 256)               │               0 │ add_5[0][0]                │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dense_7 (Dense)               │ (None, 45)                │          11,565 │ get_item[0][0]             │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
 Total params: 7,125,805 (27.18 MB)
 Trainable params: 7,125,805 (27.18 MB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12461s 400ms/step - accuracy: 0.7719 - loss: 0.8963 - val_accuracy: 0.7796 - val_loss: 0.8336 - learning_rate: 2.0000e-05
Epoch 2/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12590s 404ms/step - accuracy: 0.7752 - loss: 0.8473 - val_accuracy: 0.7796 - val_loss: 0.8394 - learning_rate: 4.0000e-05
Epoch 3/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13061s 419ms/step - accuracy: 0.7763 - loss: 0.8386 - val_accuracy: 0.7798 - val_loss: 0.8322 - learning_rate: 6.0000e-05
Epoch 4/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13066s 419ms/step - accuracy: 0.7760 - loss: 0.8337 - val_accuracy: 0.7792 - val_loss: 0.8305 - learning_rate: 8.0000e-05
Epoch 5/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12496s 401ms/step - accuracy: 0.7768 - loss: 0.8295 - val_accuracy: 0.7812 - val_loss: 0.8280 - learning_rate: 1.0000e-04
Epoch 6/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12482s 401ms/step - accuracy: 0.7762 - loss: 0.8264 - val_accuracy: 0.7809 - val_loss: 0.8236 - learning_rate: 1.0000e-04
Epoch 7/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12547s 403ms/step - accuracy: 0.7767 - loss: 0.8221 - val_accuracy: 0.7798 - val_loss: 0.8209 - learning_rate: 1.0000e-04
Epoch 8/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12646s 406ms/step - accuracy: 0.7775 - loss: 0.8173 - val_accuracy: 0.7811 - val_loss: 0.8203 - learning_rate: 1.0000e-04
Epoch 9/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13124s 421ms/step - accuracy: 0.7761 - loss: 0.8194 - val_accuracy: 0.7811 - val_loss: 0.8199 - learning_rate: 1.0000e-04
Epoch 10/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13443s 431ms/step - accuracy: 0.7782 - loss: 0.8123 - val_accuracy: 0.7808 - val_loss: 0.8164 - learning_rate: 1.0000e-04
Epoch 11/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13472s 432ms/step - accuracy: 0.7777 - loss: 0.8109 - val_accuracy: 0.7799 - val_loss: 0.8180 - learning_rate: 1.0000e-04
Epoch 12/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 0s 407ms/step - accuracy: 0.7781 - loss: 0.8087
Epoch 12: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12906s 414ms/step - accuracy: 0.7781 - loss: 0.8087 - val_accuracy: 0.7788 - val_loss: 0.8198 - learning_rate: 2.0000e-05
Epoch 13/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 12962s 416ms/step - accuracy: 0.7783 - loss: 0.8067 - val_accuracy: 0.7805 - val_loss: 0.8191 - learning_rate: 1.0000e-04
Epoch 14/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 0s 413ms/step - accuracy: 0.7785 - loss: 0.8051
Epoch 14: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13134s 422ms/step - accuracy: 0.7785 - loss: 0.8051 - val_accuracy: 0.7804 - val_loss: 0.8190 - learning_rate: 2.0000e-05
Epoch 15/50
31159/31159 ━━━━━━━━━━━━━━━━━━━━ 13229s 425ms/step - accuracy: 0.7786 - loss: 0.8028 - val_accuracy: 0.7810 - val_loss: 0.8174 - learning_rate: 1.0000e-04
C:\Users\ML_Team\Documents\Projects\MelodySynthesis>python evaluate.py
2025-03-22 18:12:42.357660: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-22 18:12:43.342215: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Loading model...
2025-03-22 18:12:47.952757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Evaluating model...
5480/5480 ━━━━━━━━━━━━━━━━━━━━ 573s 104ms/step
Model Accuracy: 0.7766
Generating visualizations...
Visualizations saved in 'plots' directory