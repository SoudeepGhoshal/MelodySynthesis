root@2c6ce095a9ee:/workspace# /bin/python3.11 /workspace/train_A2a.py
2025-05-17 06:41:46.307330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-17 06:41:46.307401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-17 06:41:46.308968: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-17 06:41:46.316701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Cleared session
Memory usage: 678.02 MB
Loading data...
Memory usage: 12469.11 MB
Train inputs shape: (997066, 64, 45), Targets shape: (997066,)
Val inputs shape: (75294, 64, 45), Targets shape: (75294,)
Data loaded.
Building hybrid model...
2025-05-17 06:42:03.818791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18325 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:81:00.0, compute capability: 8.9
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 64, 45)]             0         []                            
                                                                                                  
 dense (Dense)               (None, 64, 256)              11776     ['input_1[0][0]']             
                                                                                                  
 layer_normalization (Layer  (None, 64, 256)              512       ['dense[0][0]']               
 Normalization)                                                                                   
                                                                                                  
 multi_head_attention (Mult  (None, 64, 256)              1051904   ['layer_normalization[0][0]', 
 iHeadAttention)                                                     'layer_normalization[0][0]'] 
                                                                                                  
 dropout (Dropout)           (None, 64, 256)              0         ['multi_head_attention[0][0]']
                                                                                                  
 add (Add)                   (None, 64, 256)              0         ['dropout[0][0]',             
                                                                     'dense[0][0]']               
                                                                                                  
 layer_normalization_1 (Lay  (None, 64, 256)              512       ['add[0][0]']                 
 erNormalization)                                                                                 
                                                                                                  
 dense_1 (Dense)             (None, 64, 512)              131584    ['layer_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 dense_2 (Dense)             (None, 64, 256)              131328    ['dense_1[0][0]']             
                                                                                                  
 dropout_1 (Dropout)         (None, 64, 256)              0         ['dense_2[0][0]']             
                                                                                                  
 add_1 (Add)                 (None, 64, 256)              0         ['dropout_1[0][0]',           
                                                                     'add[0][0]']                 
                                                                                                  
 layer_normalization_2 (Lay  (None, 64, 256)              512       ['add_1[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 multi_head_attention_1 (Mu  (None, 64, 256)              1051904   ['layer_normalization_2[0][0]'
 ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]
                                                                    ']                            
                                                                                                  
 dropout_2 (Dropout)         (None, 64, 256)              0         ['multi_head_attention_1[0][0]
                                                                    ']                            
                                                                                                  
 add_2 (Add)                 (None, 64, 256)              0         ['dropout_2[0][0]',           
                                                                     'add_1[0][0]']               
                                                                                                  
 layer_normalization_3 (Lay  (None, 64, 256)              512       ['add_2[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 dense_3 (Dense)             (None, 64, 512)              131584    ['layer_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 dense_4 (Dense)             (None, 64, 256)              131328    ['dense_3[0][0]']             
                                                                                                  
 dropout_3 (Dropout)         (None, 64, 256)              0         ['dense_4[0][0]']             
                                                                                                  
 add_3 (Add)                 (None, 64, 256)              0         ['dropout_3[0][0]',           
                                                                     'add_2[0][0]']               
                                                                                                  
 layer_normalization_4 (Lay  (None, 64, 256)              512       ['add_3[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 multi_head_attention_2 (Mu  (None, 64, 256)              1051904   ['layer_normalization_4[0][0]'
 ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]
                                                                    ']                            
                                                                                                  
 dropout_4 (Dropout)         (None, 64, 256)              0         ['multi_head_attention_2[0][0]
                                                                    ']                            
                                                                                                  
 add_4 (Add)                 (None, 64, 256)              0         ['dropout_4[0][0]',           
                                                                     'add_3[0][0]']               
                                                                                                  
 layer_normalization_5 (Lay  (None, 64, 256)              512       ['add_4[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 dense_5 (Dense)             (None, 64, 512)              131584    ['layer_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 dense_6 (Dense)             (None, 64, 256)              131328    ['dense_5[0][0]']             
                                                                                                  
 dropout_5 (Dropout)         (None, 64, 256)              0         ['dense_6[0][0]']             
                                                                                                  
 add_5 (Add)                 (None, 64, 256)              0         ['dropout_5[0][0]',           
                                                                     'add_4[0][0]']               
                                                                                                  
 lstm (LSTM)                 (None, 64, 256)              525312    ['add_5[0][0]']               
                                                                                                  
 batch_normalization (Batch  (None, 64, 256)              1024      ['lstm[0][0]']                
 Normalization)                                                                                   
                                                                                                  
 lstm_1 (LSTM)               (None, 256)                  525312    ['batch_normalization[0][0]'] 
                                                                                                  
 dropout_6 (Dropout)         (None, 256)                  0         ['lstm_1[0][0]']              
                                                                                                  
 dense_7 (Dense)             (None, 45)                   11565     ['dropout_6[0][0]']           
                                                                                                  
==================================================================================================
Total params: 5022509 (19.16 MB)
Trainable params: 5021997 (19.16 MB)
Non-trainable params: 512 (2.00 KB)
__________________________________________________________________________________________________
Model built
Memory usage: 13245.94 MB
Starting training...
Epoch 1/50
2025-05-17 06:42:30.549184: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907
2025-05-17 06:42:30.708924: I external/local_xla/xla/service/service.cc:168] XLA service 0x7865280145f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2025-05-17 06:42:30.708994: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX 4000 Ada Generation, Compute Capability 8.9
2025-05-17 06:42:30.718182: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1747464150.873667    1520 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
15580/15580 [==============================] - ETA: 0s - loss: 0.9687 - accuracy: 0.7695          
Epoch 1: val_loss improved from inf to 0.90537, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 565s 36ms/step - loss: 0.9687 - accuracy: 0.7695 - val_loss: 0.9054 - val_accuracy: 0.7790 - lr: 0.0010
Epoch 2/50
15579/15580 [============================>.] - ETA: 0s - loss: 1.1661 - accuracy: 0.7312  
Epoch 2: val_loss did not improve from 0.90537
15580/15580 [==============================] - 548s 35ms/step - loss: 1.1661 - accuracy: 0.7312 - val_loss: 0.9290 - val_accuracy: 0.7795 - lr: 0.0010
Epoch 3/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.9119 - accuracy: 0.7799  
Epoch 3: val_loss did not improve from 0.90537

Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
15580/15580 [==============================] - 551s 35ms/step - loss: 0.9119 - accuracy: 0.7799 - val_loss: 0.9837 - val_accuracy: 0.7832 - lr: 2.0000e-04
Epoch 4/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7995 - accuracy: 0.7812  
Epoch 4: val_loss improved from 0.90537 to 0.76513, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 563s 36ms/step - loss: 0.7995 - accuracy: 0.7812 - val_loss: 0.7651 - val_accuracy: 0.7843 - lr: 2.0000e-04
Epoch 5/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7529 - accuracy: 0.7837  
Epoch 5: val_loss improved from 0.76513 to 0.74587, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 545s 35ms/step - loss: 0.7529 - accuracy: 0.7837 - val_loss: 0.7459 - val_accuracy: 0.7871 - lr: 2.0000e-04
Epoch 6/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.7836   
Epoch 6: val_loss improved from 0.74587 to 0.74176, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 602s 39ms/step - loss: 0.7523 - accuracy: 0.7836 - val_loss: 0.7418 - val_accuracy: 0.7877 - lr: 2.0000e-04
Epoch 7/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.7835  
Epoch 7: val_loss did not improve from 0.74176
15580/15580 [==============================] - 549s 35ms/step - loss: 0.7531 - accuracy: 0.7835 - val_loss: 0.7783 - val_accuracy: 0.7831 - lr: 2.0000e-04
Epoch 8/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7464 - accuracy: 0.7849  
Epoch 8: val_loss did not improve from 0.74176

Epoch 8: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
15580/15580 [==============================] - 547s 35ms/step - loss: 0.7464 - accuracy: 0.7849 - val_loss: 0.7865 - val_accuracy: 0.7811 - lr: 4.0000e-05
Epoch 9/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7662 - accuracy: 0.7814  
Epoch 9: val_loss improved from 0.74176 to 0.73104, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 635s 41ms/step - loss: 0.7662 - accuracy: 0.7814 - val_loss: 0.7310 - val_accuracy: 0.7879 - lr: 4.0000e-05
Epoch 10/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7342 - accuracy: 0.7851   
Epoch 10: val_loss did not improve from 0.73104
15580/15580 [==============================] - 653s 42ms/step - loss: 0.7342 - accuracy: 0.7851 - val_loss: 0.9114 - val_accuracy: 0.7677 - lr: 4.0000e-05
Epoch 11/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7947 - accuracy: 0.7767  
Epoch 11: val_loss did not improve from 0.73104

Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-05.
15580/15580 [==============================] - 636s 41ms/step - loss: 0.7947 - accuracy: 0.7767 - val_loss: 0.7380 - val_accuracy: 0.7865 - lr: 1.0000e-05
Epoch 12/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7358 - accuracy: 0.7849  
Epoch 12: val_loss did not improve from 0.73104
15580/15580 [==============================] - 544s 35ms/step - loss: 0.7358 - accuracy: 0.7849 - val_loss: 0.7335 - val_accuracy: 0.7875 - lr: 1.0000e-05
Epoch 13/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7319 - accuracy: 0.7853  
Epoch 13: val_loss improved from 0.73104 to 0.73010, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 548s 35ms/step - loss: 0.7319 - accuracy: 0.7853 - val_loss: 0.7301 - val_accuracy: 0.7869 - lr: 1.0000e-05
Epoch 14/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.7857  
Epoch 14: val_loss improved from 0.73010 to 0.72594, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 550s 35ms/step - loss: 0.7286 - accuracy: 0.7857 - val_loss: 0.7259 - val_accuracy: 0.7884 - lr: 1.0000e-05
Epoch 15/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.7858  
Epoch 15: val_loss did not improve from 0.72594
15580/15580 [==============================] - 553s 35ms/step - loss: 0.7291 - accuracy: 0.7858 - val_loss: 0.7270 - val_accuracy: 0.7880 - lr: 1.0000e-05
Epoch 16/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7326 - accuracy: 0.7851  
Epoch 16: val_loss did not improve from 0.72594
15580/15580 [==============================] - 553s 35ms/step - loss: 0.7326 - accuracy: 0.7851 - val_loss: 0.7727 - val_accuracy: 0.7833 - lr: 1.0000e-05
Epoch 17/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7361 - accuracy: 0.7849  
Epoch 17: val_loss did not improve from 0.72594
15580/15580 [==============================] - 553s 35ms/step - loss: 0.7361 - accuracy: 0.7849 - val_loss: 0.7264 - val_accuracy: 0.7885 - lr: 1.0000e-05
Epoch 18/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7327 - accuracy: 0.7851   
Epoch 18: val_loss did not improve from 0.72594
15580/15580 [==============================] - 555s 36ms/step - loss: 0.7327 - accuracy: 0.7851 - val_loss: 0.7275 - val_accuracy: 0.7882 - lr: 1.0000e-05
Epoch 19/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7272 - accuracy: 0.7860  
Epoch 19: val_loss improved from 0.72594 to 0.72475, saving model to model/A2a/hybrid_A2a.keras
15580/15580 [==============================] - 555s 36ms/step - loss: 0.7272 - accuracy: 0.7860 - val_loss: 0.7247 - val_accuracy: 0.7894 - lr: 1.0000e-05
Epoch 20/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7279 - accuracy: 0.7860  
Epoch 20: val_loss did not improve from 0.72475
15580/15580 [==============================] - 552s 35ms/step - loss: 0.7279 - accuracy: 0.7860 - val_loss: 0.7271 - val_accuracy: 0.7882 - lr: 1.0000e-05
Epoch 21/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7295 - accuracy: 0.7854  
Epoch 21: val_loss did not improve from 0.72475
15580/15580 [==============================] - 553s 35ms/step - loss: 0.7295 - accuracy: 0.7854 - val_loss: 0.7248 - val_accuracy: 0.7893 - lr: 1.0000e-05
Epoch 22/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7264 - accuracy: 0.7865  
Epoch 22: val_loss did not improve from 0.72475
15580/15580 [==============================] - 552s 35ms/step - loss: 0.7264 - accuracy: 0.7865 - val_loss: 0.7304 - val_accuracy: 0.7880 - lr: 1.0000e-05
Epoch 23/50
15580/15580 [==============================] - ETA: 0s - loss: 0.7267 - accuracy: 0.7862  
Epoch 23: val_loss did not improve from 0.72475
15580/15580 [==============================] - 553s 36ms/step - loss: 0.7267 - accuracy: 0.7862 - val_loss: 0.7264 - val_accuracy: 0.7886 - lr: 1.0000e-05
Epoch 24/50
15579/15580 [============================>.] - ETA: 0s - loss: 0.7308 - accuracy: 0.7856
Restoring model weights from the end of the best epoch: 19.

Epoch 24: val_loss did not improve from 0.72475
15580/15580 [==============================] - 555s 36ms/step - loss: 0.7308 - accuracy: 0.7856 - val_loss: 0.7270 - val_accuracy: 0.7880 - lr: 1.0000e-05
Epoch 24: early stopping
Training completed
Saving model...
Model saved successfully.
Saving training history...
Training history saved to model/A2a/training_history_A2a.json
root@2c6ce095a9ee:/workspace# /bin/python3.11 /workspace/evaluate.py
2025-05-17 10:30:01.734766: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-17 10:30:01.734833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-17 10:30:01.736381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-17 10:30:01.743915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading model...
2025-05-17 10:30:05.590866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18325 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:81:00.0, compute capability: 8.9
Evaluating model...
2025-05-17 10:30:21.395789: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907
5480/5480 [==============================] - 60s 11ms/step
Model Accuracy: 0.7846
Generating visualizations...
Training history plot generated
Visualizations saved in 'plots' directory