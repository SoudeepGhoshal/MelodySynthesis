# MelodySynthesis: LSTM-Based Melody Generation

## ğŸ“Œ Research Overview

This repository is part of an ongoing research project exploring deep learning approaches for melody synthesis. The study compares different architecturesâ€”including **LSTM, Transformer, a Transformer-LSTM hybrid, and a GAN-based Transformer-LSTM hybrid model**â€”to determine which performs best in generating musically coherent melodies.  

This branch (`lstm`) focuses specifically on **LSTM-based melody generation**, investigating how recurrent neural networks model temporal dependencies in musical sequences.  

## ğŸ“‚ Repository Structure

```
MelodySynthesis/
â”‚â”€â”€ melodies/               # Datasets used for training & evaluation
â”‚â”€â”€ processed_data/         # Preprocessed data formatted for model input
â”‚â”€â”€ model/                  # Trained models & related artifacts
â”‚â”€â”€ plots/                  # Training analysis & model evaluation plots
â”‚â”€â”€ src/
â”‚   â”‚â”€â”€ train.py            # LSTM model training script
â”‚   â”‚â”€â”€ melody_generator.py # Script to generate melodies using trained model
â”‚   â”‚â”€â”€ evaluate.py         # Performance evaluation metrics & visualization
â”‚   â”‚â”€â”€ utils.py            # Data preprocessing & helper functions
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ README.md               # Project documentation (this file)
â”‚â”€â”€ LICENSE                 # License information
```

## ğŸ¶ Model Architecture

The **LSTM model** is designed to generate sequences of musical notes based on training data. The architecture follows a sequential structure:  

- **Input Layer**: Sequences of musical notes encoded as vectors.  
- **LSTM Layer 1**: 256 units, returns sequences for further processing.  
- **Batch Normalization**: Normalizes activations to stabilize training.  
- **LSTM Layer 2**: 256 units, outputs a fixed-length representation.  
- **Dropout Layer**: Applied (rate=0.5) to prevent overfitting.  
- **Dense Output Layer**: Predicts the next note in the sequence.  

ğŸ”¹ **Total Trainable Parameters**: 846,637  

Model visualization:  
![Model Architecture](model/model_architecture.png)  

## ğŸ“Š Training Details

**Training Parameters**:
- Optimizer: **Adam** (`lr=0.001`, adaptive learning rate)
- Loss Function: **Categorical Cross-Entropy**
- Batch Size: **64**
- Epochs: **50** (with **early stopping** to prevent overfitting)

Training stopped at **epoch 21**, achieving:
- **Training Accuracy**: **87.61%**
- **Validation Accuracy**: **85.26%**
- **Test Accuracy**: **85.36%**

**Training Progress** (loss & accuracy over epochs):  
![Training Analysis](model/plots/training_history.png)  

## ğŸ“ˆ Model Performance & Evaluation  

The trained LSTM model was evaluated using standard classification metrics:  

âœ”ï¸ **Confusion Matrix**:  
![Confusion Matrix](model/plots/confusion_matrix.png)  

âœ”ï¸ **Classification Counts**:  
![Classification Distribution](model/plots/classification_counts.png)  

### Key Observations:
- The LSTM successfully captures musical structure but exhibits slight overfitting.
- Some note sequences are more challenging to predict, as seen in the confusion matrix.
- The model maintains consistency across train, validation, and test sets.

## ğŸ› ï¸ How to Use

### 1ï¸âƒ£ Setup Environment  
Ensure dependencies are installed:  
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Train the Model  
If you wish to retrain the model, run:  
```bash
python train.py
```

### 3ï¸âƒ£ Generate Melodies  
Run the melody generation script:  
```bash
python melody_generator.py
```
Generated melodies will be saved in the `generated_melodies/` folder.

### 4ï¸âƒ£ Evaluate the Model  
Analyze the model's performance using:  
```bash
python evaluate.py
```

## ğŸ’ª Research Significance  

This study contributes to **AI-driven music generation** by comparing deep learning architectures for melody synthesis. Our findings will:  
- Identify **which model best captures musical structure**.
- Explore how **recurrent networks** compare with **attention-based models** in sequence generation.
- Provide insights into **hybrid architectures (Transformer-LSTM, GAN-based models)** for music synthesis.

Each model is implemented in a separate branch:  
- **LSTM Model** (`lstm` branch) â¬…ï¸ *(current branch)*  
- **Transformer Model** (`transformer` branch)  
- **Hybrid Transformer-LSTM** (`hybrid-transformer-lstm` branch)  
- **GAN-Based Model** (`gan-transformer-lstm` branch)  

By analyzing these models, we aim to determine the most **effective approach for AI-generated melodies**.  

## ğŸš€ Future Work

- ğŸ”¹ Implementing **Transformer & hybrid models** (comparison across architectures)
- ğŸ”¹ Exploring **music-theory-guided generation** (ensuring harmony & rhythm)
- ğŸ”¹ Enhancing **real-time generation capabilities**

## ğŸ¤ Contributors

- **Soudeep Ghoshal** ([@SoudeepGhoshal](https://github.com/SoudeepGhoshal))   

## ğŸ License  

This project is licensed under the **MIT License**. See [LICENSE](LICENSE.txt) for details.  

---  
ğŸ“Œ **Note**: This is a research-oriented repository, and findings will be documented in an upcoming paper. If you use this work, please cite appropriately.