# MelodySynthesis: Transformer-Based Melody Generation

## ğŸ“Œ Research Overview

This repository is part of an ongoing research project exploring deep learning approaches for melody synthesis. The study compares different architecturesâ€”including **LSTM, Transformer, a Transformer-LSTM hybrid, and a GAN-based Transformer-LSTM hybrid model**â€”to determine which performs best in generating musically coherent melodies.

This branch (`transformer`) focuses specifically on **Transformer-based melody generation**, investigating how self-attention mechanisms capture musical dependencies in sequences more efficiently than recurrence-based approaches.

## ğŸ“‚ Repository Structure

```
MelodySynthesis/
â”‚â”€â”€ melodies/               # Performance evaluation files
â”‚â”€â”€ processed_data/         # Preprocessed data formatted for model input
â”‚â”€â”€ model/                  # Trained models & related artifacts
â”‚   â”‚â”€â”€ plots/              # Training analysis & model evaluation plots
â”‚â”€â”€ train.py                # Transformer model training script
â”‚â”€â”€ melody_generator.py     # Script to generate melodies using trained model
â”‚â”€â”€ evaluate.py             # Performance evaluation metrics & visualization
â”‚â”€â”€ utils.py                # Data preprocessing & helper functions
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ README.md               # Project documentation (this file)
â”‚â”€â”€ LICENSE.txt             # License information
```

## ğŸ¶ Model Architecture

The **Transformer model** uses self-attention layers to understand global patterns in musical sequences. The model's decoder stack comprises multiple layers of attention and feedforward blocks.

### Transformer Decoder Block

The decoder block consists of the following key components, arranged sequentially:

1. **Layer Normalization** â€“ Normalizes the inputs to stabilize and speed up training.
2. **Multi-Head Attention** â€“ Captures dependencies across different positions in the sequence using multiple attention heads.
3. **Dropout** â€“ Applied for regularization to prevent overfitting.
4. **Residual Connection** â€“ Adds the attention output back to the original input to preserve learned features.
5. **Another Layer Normalization** â€“ Normalizes the result again before passing it to the feedforward network.
6. **Position-wise Feedforward Network** â€“ Comprises two Dense layers with ReLU activation in between to introduce non-linearity.
7. **Second Dropout Layer** â€“ Further regularization.
8. **Final Residual Connection** â€“ Adds the feedforward output back to the earlier residual output for stability and gradient flow.

The full model stacks three of these decoder blocks after projecting input through a Dense layer. A final Dense layer with softmax activation generates probability distributions over output notes.

This structure helps the model effectively model long-term dependencies and hierarchical representations in musical sequences.

Model visualization:  
![Model Architecture](model/transformer_architecture.png)

## ğŸ“Š Training Details

**Training Parameters**:
- Optimizer: **Adam** with a **Custom Learning Rate Schedule**:  
  A custom learning rate schedule is used instead of a fixed rate. The learning rate increases linearly for the first few thousand steps (warm-up phase) and then decays proportionally to the inverse square root of the step number. This helps stabilize and accelerate training, especially in attention-based models. The schedule is configured with ```d_model = 256``` and ```warmup_steps = 4000```.
  
- Loss Function: **Sparse Categorical Cross-Entropy**
- Batch Size: **64**
- Epochs: **50** (with **early stopping** to prevent overfitting)


Training stopped at **epoch 18** (best model restored from **epoch 13**), achieving:
- **Training Accuracy**: **0.8557**
- **Validation Accuracy**: **0.8481**
- **Test Accuracy**: **0.8482**

**Training Progress** (accuracy & loss over epochs):  
![Training Analysis](model/plots/training_history.png)

## ğŸ“ˆ Model Performance & Evaluation

The trained Transformer model was evaluated using standard classification metrics:

âœ”ï¸ **Confusion Matrix**:  
![Confusion Matrix](model/plots/confusion_matrix.png)

âœ”ï¸ **Classification Counts**:  
![Classification Distribution](model/plots/classification_counts.png)

### Key Observations:
- The Transformer captures long-term dependencies more effectively than RNNs.
- Some note sequences still pose prediction challenges.
- Minimal overfitting observed with appropriate regularization.

## ğŸ¼ Evaluation of Generated Melodies

To assess the quality of melodies generated by the Transformer model, 1000 melodies were sampled from random seed sequences and evaluated using various musical metrics. Below are key evaluation results:

| **Metric**                | **Mean** | **Min** | **Max** |
| ------------------------- | -------: | ------: | ------: |
| **Pitch Variance**        |   3.6332 |    0.25 | 13.9213 |
| **Pitch Range**           |   4.3260 |  0.5714 |    10.5 |
| **Rhythmic Variance**     |   0.1022 |       0 |  0.6600 |
| **Note Density**          |   1.1394 |  0.3333 |  3.0160 |
| **Rest Ratio**            |   0.0925 |       0 |  0.3750 |
| **Interval Variability**  |   1.6892 |       0 | 14.2550 |
| **Note Repetition**       |   1.8957 |     1.0 |  5.0476 |
| **Contour Stability**     |   0.6474 |       0 |     3.4 |
| **Syncopation**           |   1.6469 |       0 |     9.5 |
| **Harmonic Tension**      |   0.4351 |  0.1650 |  0.6897 |
| **KL Divergence**         |   0.7652 |  0.0079 |  1.9620 |
| **Pitch Entropy**         |   2.4306 |  0.8925 |  3.4053 |
| **Rhythmic Entropy**      |   1.2013 |       0 |  2.5029 |
| **Motif Diversity Index** |   0.7414 |  0.0690 |     1.0 |
| **Harmonic Complexity**   |   0.5081 |  0.0944 |  1.3302 |
| **Contour Variability**   |  24.1560 |       0 | 64.2857 |
| **Tonal Drift**           |   3.5770 |       0 |    12.0 |

## ğŸ› ï¸ How to Use

### 1ï¸âƒ£ Setup Environment
Install the required dependencies:
```bash
pip install -r requirements.txt
```
Download the pre-trained model:
```bash
gdown https://drive.google.com/uc?id=1BJf8oagKfX9rsiMxgkmf7uGM6IkUTIVZ --output model/transformer.keras
```

### 2ï¸âƒ£ Train the Model
```bash
python train.py
```

### 3ï¸âƒ£ Generate Melodies
```bash
python melody_generator.py
```

### 4ï¸âƒ£ Evaluate the Model
```bash
python evaluate.py
```

## ğŸ’ª Research Significance

This study contributes to **AI-driven music generation** by comparing deep learning architectures for melody synthesis. Our findings will:
- Identify **which model best captures musical structure**.
- Explore how **attention-based models** compare with **recurrent networks** in sequence generation.
- Provide insights into **hybrid architectures (Transformer-LSTM, GAN-based models)** for music synthesis.

Each model is implemented in a separate branch:
- **LSTM Model** (`lstm` branch)
- **Transformer Model** (`transformer` branch) â¬…ï¸ *(current branch)*
- **Hybrid Transformer-LSTM** (`hybrid-transformer-lstm` branch)
- **GAN-Based Model** (`gan-transformer-lstm` branch)

By analyzing these models, we aim to determine the most **effective approach for AI-generated melodies**.

## ğŸš€ Future Work

- ğŸ”¹ Comparing performance across all four architectures
- ğŸ”¹ Incorporating **music-theory-based constraints**
- ğŸ”¹ Real-time music generation and streaming

## ğŸ¤ Contributors

- **Soudeep Ghoshal** ([@SoudeepGhoshal](https://github.com/SoudeepGhoshal))
- **Sandipan Chakraborty** ([@SChakraborty04](https://github.com/SChakraborty04))

## ğŸ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE.txt) for details.

---
ğŸ“Œ **Note**: This is a research-oriented repository. Please cite this work appropriately if you use it.